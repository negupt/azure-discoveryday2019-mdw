{"cells":[{"cell_type":"markdown","source":["# Azure Discovery Day 2019\n## Analytics with NRT Intelligence on Azure\n\n#### Summary\nIn this Python Jupyter notebook, you will:\n1. Connect to Azure storage\n2. Ingest data from CSV files in Azure storage to Spark dataframes\n3. Conform and merge heterogenous data sets using the Spark dataframe API\n4. Emit data to Azure storage in Parquet file format\n\nAdditionally, there are optional steps to create Hive tables on the data, query them with Spark SQL, as well as some exploratory data analysis (EDA)."],"metadata":{}},{"cell_type":"code","source":["## Need some library includes\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import broadcast, lit"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Functions"],"metadata":{}},{"cell_type":"code","source":["## Function to get a Spark DataFrame from a CSV source file\n\ndef GetDataFrameFromCsvFile(schema, sourceFilePath, delimiter):\n  df = spark\\\n    .read\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"delimiter\", delimiter)\\\n    .schema(schema)\\\n    .load(sourceFilePath)\n  \n  return df;"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["## Given a reference dataframe (this would not make sense for large transaction dataframes), broadcast it across the cluster, lazy-cache it, and return the count, which instantiates the dataframe\n\ndef HandleReferenceDataFrame(df):\n  broadcast(df)\n  df.cache()\n  count = df.count()\n  \n  return count;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["## Delete Spark job residual files (_SUCCESS, _start*, _committed*) down the folder/file hierarchy\n\nimport os\n\ndef CleanupSparkJobFiles(parquetFolderPath):\n  file_paths = GetFilesRecursive(parquetFolderPath)\n  \n  for file_path in file_paths:\n    # Get just the file name\n    file_name = os.path.basename(file_path)\n    # print(file_name)\n    \n    if file_name.startswith(\"_\"):\n      # Temp job file - delete it\n      dbutils.fs.rm(file_path)\n    # elif file_name.endswith(\".parquet\"):\n      # Data file - no op\n    # else:\n      # Something else - no op"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["## Get iterable file list. Flattens hierarchical folder/file structure.\n\ndef GetFilesRecursive(rootPath):\n  final_list = []\n\n  for directoryItem in dbutils.fs.ls(rootPath):\n    directoryItemPathClean = directoryItem.path.replace(\"%25\", \"%\").replace(\"%25\", \"%\")\n    \n    if directoryItem.isDir() == True:\n      final_list = final_list + GetFilesRecursive(directoryItemPathClean)\n    else:\n      final_list.append(directoryItemPathClean)\n  \n  return final_list;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### Connect to Azure Storage"],"metadata":{}},{"cell_type":"code","source":["# Define some variables to minimize \"hard-coding\" in below cells. Note that variables can also be defined in a separate notebook.\n\n# Azure storage account information\nstorageAcctName = \"PROVIDE\"\nstorageAcctKey = \"PROVIDE\"\ncontainerName = \"PROVIDE\"\n\n# The mount point in the DBFS file system - this will look like a local folder but points to the Azure storage location\nmountPoint = \"/mnt/\" + containerName + \"/\""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["## Use the Databricks file system utilities to mount a Databricks file system location (/mnt/YOUR CONTAINER NAME) that points to the Azure storage account where data files are located\n## We use variables defined above and string concatenation here so that no \"hard-coding\" is needed\n\ndbutils.fs.mount(\n  source = \"wasbs://\" + containerName + \"@\" + storageAcctName + \".blob.core.windows.net\",\n  mount_point = mountPoint,\n  extra_configs = {\"fs.azure.account.key.\" + storageAcctName + \".blob.core.windows.net\":storageAcctKey}\n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["## This is included to remove the Azure storage mount\n## Commented out since not needed for the lab, but included here \"just in case\" for debugging/experimenting - for example, mount, unmount, try something different, mount again\n\n#dbutils.fs.unmount(mountPoint)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["## List contents of the Azure storage account to validate successful connect and mount\n## We are using the Databricks display() function here to improve the esthetics of the output\n\ndisplay(dbutils.fs.ls(mountPoint))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Load Reference Data Files into DataFrames"],"metadata":{}},{"cell_type":"markdown","source":["##### Define variables to hold the source path for each of the reference data files"],"metadata":{}},{"cell_type":"code","source":["src_file_ref_payment_type = mountPoint + \"reference-data/payment_type_lookup.csv\"\nsrc_file_ref_rate_code = mountPoint + \"reference-data/rate_code_lookup.csv\"\nsrc_file_ref_taxi_zone = mountPoint + \"reference-data/taxi_zone_lookup.csv\"\nsrc_file_ref_trip_month = mountPoint + \"reference-data/trip_month_lookup.csv\"\nsrc_file_ref_trip_type = mountPoint + \"reference-data/trip_type_lookup.csv\"\nsrc_file_ref_vendor = mountPoint + \"reference-data/vendor_lookup.csv\""],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["##### Define explicit schemas for each of the reference data files\n\nWe could also ingest files with schema inference (i.e. tell Spark to try to figure it out) but let's be explicit here for greater control."],"metadata":{}},{"cell_type":"code","source":["## Payment type\nschema_ref_payment_type = StructType([\n    StructField(\"payment_type\", IntegerType(), True),\n    StructField(\"abbreviation\", StringType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n## Rate code ID\nschema_ref_rate_code = StructType([\n    StructField(\"rate_code_id\", IntegerType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n## Taxi zone\nschema_ref_taxi_zone = StructType([\n    StructField(\"location_id\", StringType(), True),\n    StructField(\"borough\", StringType(), True),\n    StructField(\"zone\", StringType(), True),\n    StructField(\"service_zone\", StringType(), True)\n])\n\n## Trip month\nschema_ref_trip_month = StructType([\n    StructField(\"trip_month\", StringType(), True),\n    StructField(\"month_name_short\", StringType(), True),\n    StructField(\"month_name_full\", StringType(), True)\n])\n\n## Trip type\nschema_ref_trip_type = StructType([\n    StructField(\"trip_type\", IntegerType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n## Vendor ID\nschema_ref_vendor = StructType([\n    StructField(\"vendor_id\", IntegerType(), True),\n    StructField(\"abbreviation\", StringType(), True),\n    StructField(\"description\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["##### Load each reference data set into a Spark DataFrame\n\nWe load the data from source file into dataframe using a function (above) for that purpose.\n\nThen we do some more optimizations for the reference dataframes:\n1. Broadcast the dataframe. These are small dataframes with reference data. Broadcasting means we replicate a dataframe to each worker node in a Spark cluster, so that cross-node (cross-network) joins are avoided.\n2. Lazy-cache the dataframe into memory as another performance optimization.\n\nLast, we print the number rows in the dataframe."],"metadata":{}},{"cell_type":"code","source":["df_ref_payment_type = GetDataFrameFromCsvFile(schema_ref_payment_type, src_file_ref_payment_type, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_payment_type))\ndisplay(df_ref_payment_type)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df_ref_rate_code = GetDataFrameFromCsvFile(schema_ref_rate_code, src_file_ref_rate_code, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_rate_code))\ndisplay(df_ref_rate_code)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["df_ref_taxi_zone = GetDataFrameFromCsvFile(schema_ref_taxi_zone, src_file_ref_taxi_zone, \",\")\n\nprint(HandleReferenceDataFrame(df_ref_taxi_zone))\ndisplay(df_ref_taxi_zone)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df_ref_trip_month = GetDataFrameFromCsvFile(schema_ref_trip_month, src_file_ref_trip_month, \",\")\n\nprint(HandleReferenceDataFrame(df_ref_trip_month))\ndisplay(df_ref_trip_month)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["df_ref_trip_type = GetDataFrameFromCsvFile(schema_ref_trip_type, src_file_ref_trip_type, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_trip_type))\ndisplay(df_ref_trip_type)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["df_ref_vendor = GetDataFrameFromCsvFile(schema_ref_vendor, src_file_ref_vendor, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_vendor))\ndisplay(df_ref_vendor)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Write reference data out to Parquet files\n\nParquet files are faster to load than CSV. They also support partitioning, but for the small reference data files, we coalesce the dataframe to 1 piece and we do not partition."],"metadata":{}},{"cell_type":"code","source":["## Define the root path where we will write all Parquet data files\npath_parquet = mountPoint + \"parquet/\"\n\n## Cleanup - delete the Parquet folder if it's present\ndbutils.fs.rm(path_parquet, True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">82</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["df_ref_payment_type.coalesce(1).write.parquet(path_parquet + \"ref-payment-type/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["df_ref_rate_code.coalesce(1).write.parquet(path_parquet + \"ref-rate-code/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["df_ref_taxi_zone.coalesce(1).write.parquet(path_parquet + \"ref-taxi-zone/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["df_ref_trip_month.coalesce(1).write.parquet(path_parquet + \"ref-trip-month/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["df_ref_trip_type.coalesce(1).write.parquet(path_parquet + \"ref-trip-type/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["df_ref_vendor.coalesce(1).write.parquet(path_parquet + \"ref-vendor/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["## Delete Spark job files recursively\n\nCleanupSparkJobFiles(path_parquet)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":34}],"metadata":{"name":"lab1","notebookId":3103873265031220},"nbformat":4,"nbformat_minor":0}
