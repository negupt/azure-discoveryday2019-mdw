{"cells":[{"cell_type":"markdown","source":["# Azure Discovery Day 2019\n## Analytics with NRT Intelligence on Azure\n\n#### Summary\nIn this Python Jupyter notebook, you will:\n1. Connect to Azure storage\n2. Ingest data from CSV files in Azure storage to Spark dataframes\n3. Conform and merge heterogenous data sets using the Spark dataframe API\n4. Emit data to Azure storage in Parquet file format\n\nAdditionally, there are optional steps to create Hive tables on the data, query them with Spark SQL, as well as some exploratory data analysis (EDA)."],"metadata":{}},{"cell_type":"code","source":["## Need some library includes\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import broadcast, lit"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Functions"],"metadata":{}},{"cell_type":"code","source":["## Function to get a Spark DataFrame from a CSV source file\n\ndef GetDataFrameFromCsvFile(schema, sourceFilePath, delimiter):\n  df = spark\\\n    .read\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"delimiter\", delimiter)\\\n    .schema(schema)\\\n    .load(sourceFilePath)\n  \n  return df;"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def HandleReferenceDataFrame(df):\n  broadcast(df)\n  df.cache()\n  count = df.count()\n  \n  return count;"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Connect to Azure Storage"],"metadata":{}},{"cell_type":"code","source":["# Define some variables to minimize \"hard-coding\" in below cells. Note that variables can also be defined in a separate notebook.\n\n# Azure storage account information\nstorageAcctName = \"PROVIDE\"\nstorageAcctKey = \"PROVIDE\"\ncontainerName = \"PROVIDE\"\n\n# The mount point in the DBFS file system - this will look like a local folder but points to the Azure storage location\nmountPoint = \"/mnt/\" + containerName + \"/\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["## Use the Databricks file system utilities to mount a Databricks file system location (/mnt/YOUR CONTAINER NAME) that points to the Azure storage account where data files are located\n## We use variables defined above and string concatenation here so that no \"hard-coding\" is needed\n\ndbutils.fs.mount(\n  source = \"wasbs://\" + containerName + \"@\" + storageAcctName + \".blob.core.windows.net\",\n  mount_point = mountPoint,\n  extra_configs = {\"fs.azure.account.key.\" + storageAcctName + \".blob.core.windows.net\":storageAcctKey}\n)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["## This is included to remove the Azure storage mount\n## Commented out since not needed for the lab, but included here \"just in case\" for debugging/experimenting - for example, mount, unmount, try something different, mount again\n\n#dbutils.fs.unmount(mountPoint)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["## List contents of the Azure storage account to validate successful connect and mount\n## We are using the Databricks display() function here to improve the esthetics of the output\n\ndisplay(dbutils.fs.ls(mountPoint))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Load Reference Data Files into DataFrames"],"metadata":{}},{"cell_type":"markdown","source":["##### Define variables to hold the source path for each of the reference data files"],"metadata":{}},{"cell_type":"code","source":["src_file_ref_payment_type = mountPoint + \"reference-data/payment_type_lookup.csv\"\nsrc_file_ref_rate_code = mountPoint + \"reference-data/rate_code_lookup.csv\"\nsrc_file_ref_taxi_zone = mountPoint + \"reference-data/taxi_zone_lookup.csv\"\nsrc_file_ref_trip_month = mountPoint + \"reference-data/trip_month_lookup.csv\"\nsrc_file_ref_trip_type = mountPoint + \"reference-data/trip_type_lookup.csv\"\nsrc_file_ref_vendor = mountPoint + \"reference-data/vendor_lookup.csv\""],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["##### Define explicit schemas for each of the reference data files\n\nWe could also ingest files with schema inference (i.e. tell Spark to try to figure it out) but let's be explicit here for greater control."],"metadata":{}},{"cell_type":"code","source":["## Payment type\nschema_ref_payment_type = StructType([\n    StructField(\"payment_type\", IntegerType(), True),\n    StructField(\"abbreviation\", StringType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n## Rate code ID\nschema_ref_rate_code = StructType([\n    StructField(\"rate_code_id\", IntegerType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n## Taxi zone\nschema_ref_taxi_zone = StructType([\n    StructField(\"location_id\", StringType(), True),\n    StructField(\"borough\", StringType(), True),\n    StructField(\"zone\", StringType(), True),\n    StructField(\"service_zone\", StringType(), True)\n])\n\n## Trip month\nschema_ref_trip_month = StructType([\n    StructField(\"trip_month\", StringType(), True),\n    StructField(\"month_name_short\", StringType(), True),\n    StructField(\"month_name_full\", StringType(), True)\n])\n\n## Trip type\nschema_ref_trip_type = StructType([\n    StructField(\"trip_type\", IntegerType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n## Vendor ID\nschema_ref_vendor = StructType([\n    StructField(\"vendor_id\", IntegerType(), True),\n    StructField(\"abbreviation\", StringType(), True),\n    StructField(\"description\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["##### Load each reference data set into a Spark DataFrame\n\nWe load the data from source file into dataframe using a function (above) for that purpose.\n\nThen we do some more optimizations for the reference dataframes:\n1. Broadcast the dataframe. These are small dataframes with reference data. Broadcasting means we replicate a dataframe to each worker node in a Spark cluster, so that cross-node (cross-network) joins are avoided.\n2. Lazy-cache the dataframe into memory as another performance optimization.\n\nLast, we print the number rows in the dataframe."],"metadata":{}},{"cell_type":"code","source":["df_ref_payment_type = GetDataFrameFromCsvFile(schema_ref_payment_type, src_file_ref_payment_type, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_payment_type))\ndisplay(df_ref_payment_type)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df_ref_rate_code = GetDataFrameFromCsvFile(schema_ref_rate_code, src_file_ref_rate_code, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_rate_code))\ndisplay(df_ref_rate_code)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["df_ref_taxi_zone = GetDataFrameFromCsvFile(schema_ref_taxi_zone, src_file_ref_taxi_zone, \",\")\n\nprint(HandleReferenceDataFrame(df_ref_taxi_zone))\ndisplay(df_ref_taxi_zone)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df_ref_trip_month = GetDataFrameFromCsvFile(schema_ref_trip_month, src_file_ref_trip_month, \",\")\n\nprint(HandleReferenceDataFrame(df_ref_trip_month))\ndisplay(df_ref_trip_month)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["df_ref_trip_type = GetDataFrameFromCsvFile(schema_ref_trip_type, src_file_ref_trip_type, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_trip_type))\ndisplay(df_ref_trip_type)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df_ref_vendor = GetDataFrameFromCsvFile(schema_ref_vendor, src_file_ref_vendor, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_vendor))\ndisplay(df_ref_vendor)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"lab1","notebookId":3103873265031220},"nbformat":4,"nbformat_minor":0}
